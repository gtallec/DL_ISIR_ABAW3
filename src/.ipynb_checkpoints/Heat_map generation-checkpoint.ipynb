{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1ce8fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering dataset metric constructor\n"
     ]
    }
   ],
   "source": [
    "from models.interface import get_model\n",
    "from datasets.bp4d.pandas_interface import columns_bp4d\n",
    "from metrics_extended.interface import get_metrics\n",
    "from datasets.bp4d.generation import gen_bp4d\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "bp4d_model_dict = dict({\n",
    "    \"main\": {\n",
    "        \"type\": \"encoder_regressor\",\n",
    "        \"dependencies\": [\n",
    "            \"encoder\",\n",
    "            \"regressor\"\n",
    "        ]\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"type\": \"inceptionv3_encoder\",\n",
    "        \"optimizer\": \"encoder\",\n",
    "        \"pooling\": \"max\",\n",
    "        \"weights\": \"imagenet\"\n",
    "    },\n",
    "    \"regressor\": {\n",
    "        \"type\": \"imonet\",\n",
    "        \"n_task\": 12,\n",
    "        \"n_permutations\": 512,\n",
    "        \"drop_out\": 128,\n",
    "        \"N_sample\": 10,\n",
    "        \"label_units\": 64,\n",
    "        \"permutation_units\": 64,\n",
    "        \"dependencies\": [\n",
    "            \"vector\"\n",
    "        ],\n",
    "        \"permutation_heuristic\": {\n",
    "            \"type\": \"random\"\n",
    "        },\n",
    "        \"recurrent_cell_args\": {\n",
    "            \"type\": \"gruv2\",\n",
    "            \"units\": 64\n",
    "        },\n",
    "        \"optimizer\": \"regressor\"\n",
    "    },\n",
    "    \"vector\": {\n",
    "        \"type\": \"vector\",\n",
    "        \"n_permutations\": 512,\n",
    "        \"optimizer\": \"permutation\"\n",
    "    }\n",
    "})\n",
    "\n",
    "bp4d_metrics_dict = [\n",
    "    {\n",
    "        \"type\": \"auc_roc\",\n",
    "        \"num_thresholds\": 200,\n",
    "        \"pred_in\": \"global_pred\",\n",
    "        \"n_coords\": 12\n",
    "    }\n",
    "]\n",
    "bp4d_columns = columns_bp4d(['AU_binary'])\n",
    "metrics = get_metrics(bp4d_metrics_dict, bp4d_columns, log_folder=None)\n",
    "\n",
    "model = get_model(bp4d_model_dict, frequencies=tf.constant(tf.ones((12, ))))\n",
    "model.build((None, 299, 299, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4da1b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = os.path.join('..', 'logs', 'CVPR_submission', 'bp4d_au', 'monet_fwbp', 'iv3', 'fold0', '9041',\n",
    "                           'checkpoints', '2-epoch')\n",
    "model.load_weights(ckpt_path=weight_path,\n",
    "                   block=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41010487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mixture': <tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
       " array([0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312, 0.00195312, 0.00195312, 0.00195312,\n",
       "        0.00195312, 0.00195312], dtype=float32)>,\n",
       " 'permutation_matrix': <tf.Tensor: shape=(12, 12), dtype=float32, numpy=\n",
       " array([[0.06640625, 0.08203125, 0.08007812, 0.08398438, 0.07617188,\n",
       "         0.09570312, 0.09179688, 0.06640625, 0.08398438, 0.09179688,\n",
       "         0.10351562, 0.078125  ],\n",
       "        [0.10742188, 0.08984375, 0.06640625, 0.08398438, 0.08007812,\n",
       "         0.0859375 , 0.09570312, 0.07617188, 0.08398438, 0.07226562,\n",
       "         0.07226562, 0.0859375 ],\n",
       "        [0.04882812, 0.09765625, 0.10742188, 0.06835938, 0.09179688,\n",
       "         0.07421875, 0.0859375 , 0.09179688, 0.08789062, 0.08984375,\n",
       "         0.07617188, 0.08007812],\n",
       "        [0.06835938, 0.08203125, 0.06445312, 0.078125  , 0.08203125,\n",
       "         0.0703125 , 0.08984375, 0.08984375, 0.10351562, 0.09375   ,\n",
       "         0.08984375, 0.08789062],\n",
       "        [0.09375   , 0.06835938, 0.07617188, 0.09960938, 0.07421875,\n",
       "         0.09179688, 0.05078125, 0.10742188, 0.08398438, 0.09765625,\n",
       "         0.08789062, 0.06835938],\n",
       "        [0.078125  , 0.07617188, 0.08007812, 0.08789062, 0.09179688,\n",
       "         0.08789062, 0.078125  , 0.1015625 , 0.08398438, 0.07421875,\n",
       "         0.07617188, 0.08398438],\n",
       "        [0.10546875, 0.08203125, 0.06835938, 0.09179688, 0.08984375,\n",
       "         0.10351562, 0.11132812, 0.05273438, 0.06640625, 0.08398438,\n",
       "         0.06445312, 0.08007812],\n",
       "        [0.06640625, 0.07226562, 0.0703125 , 0.07617188, 0.09179688,\n",
       "         0.08203125, 0.078125  , 0.09375   , 0.09570312, 0.09375   ,\n",
       "         0.09179688, 0.08789062],\n",
       "        [0.10351562, 0.09570312, 0.09960938, 0.08789062, 0.06054688,\n",
       "         0.08398438, 0.078125  , 0.08007812, 0.06445312, 0.07421875,\n",
       "         0.0859375 , 0.0859375 ],\n",
       "        [0.07617188, 0.09179688, 0.11328125, 0.06445312, 0.0625    ,\n",
       "         0.06054688, 0.08789062, 0.08007812, 0.10351562, 0.078125  ,\n",
       "         0.09765625, 0.08398438],\n",
       "        [0.08398438, 0.07421875, 0.09375   , 0.09375   , 0.08789062,\n",
       "         0.0703125 , 0.08398438, 0.08789062, 0.06835938, 0.08398438,\n",
       "         0.07421875, 0.09765625],\n",
       "        [0.1015625 , 0.08789062, 0.08007812, 0.08398438, 0.11132812,\n",
       "         0.09375   , 0.06835938, 0.07226562, 0.07421875, 0.06640625,\n",
       "         0.08007812, 0.08007812]], dtype=float32)>,\n",
       " 'mixture_logits': <tf.Tensor: shape=(1, 512), dtype=float32, numpy=\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       dtype=float32)>,\n",
       " 'global_pred': <tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
       " array([[0.27402195, 0.295238  , 0.2954302 , 0.6109294 , 0.6662297 ,\n",
       "         0.6212949 , 0.5756652 , 0.47673234, 0.33475196, 0.47733435,\n",
       "         0.32299474, 0.3037978 ]], dtype=float32)>,\n",
       " 'input': <tf.Tensor: shape=(1, 299, 299, 3), dtype=float32, numpy=\n",
       " array([[[[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]]], dtype=float32)>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tf.ones((1, 299, 299, 3)), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0527e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1582it [05:44,  4.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "bp4d = gen_bp4d(fold='fold0', labels=['AU_binary'], batchsize=32)\n",
    "\n",
    "def eval_step(inputs, targets, metrics):\n",
    "    y_pred = model(inputs, training=False)\n",
    "    metrics.update_state(targets, y_pred)\n",
    "    return eval_step\n",
    "\n",
    "for (eval_inputs, eval_labels) in tqdm(bp4d): \n",
    "    eval_step(inputs=eval_inputs,\n",
    "              targets=eval_labels,\n",
    "              metrics=metrics) \n",
    "result = metrics.result_to_df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29de0182",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
